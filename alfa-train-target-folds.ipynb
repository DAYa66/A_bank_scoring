{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом ноутбуке разбивал данные на фолды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:34:03.858924Z",
     "iopub.status.busy": "2023-03-26T05:34:03.858462Z",
     "iopub.status.idle": "2023-03-26T05:34:03.952267Z",
     "shell.execute_reply": "2023-03-26T05:34:03.950095Z",
     "shell.execute_reply.started": "2023-03-26T05:34:03.858887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/sample_submission.csv\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/description.xlsx\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_target.csv\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/test_target.csv\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/test_data/test_data_0.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/test_data/test_data_1.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_0.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_9.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_5.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_4.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_2.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_10.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_3.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_8.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_6.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_11.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_1.pq\n",
      "/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data/train_data_7.pq\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_6.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_2.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_3.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_7.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_6.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_2.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_4.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_3.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_5.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_7.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_4.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_1.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/val_target_fold_5.csv\n",
      "/kaggle/input/alfa-bank-train-target-folds/train_target_fold_1.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports and requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для моделирования нам понадобится реализация градиентного бустинга из `lightgbm`. Для обработки данных &ndash; стандартный DS стек из `pandas`, `numpy` и `sklearn`. Так как мы будем использовать достаточно объемные выборки данных (кредитные истории клиентов), то нужно будет эффективно читать данные и выделять из них признаки, чтобы иметь возможность строить решение даже на локальных машинах с ограничениями по оперативной памяти.  Поэтому нам понадобятся инструменты для работы с форматом данных `Parquet`. Библиотека `pandas` представляет необходимый инструментарий (рекомендуем установить модуль `fastparquet`, это позволит еще быстрее считывать данные)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:03.955658Z",
     "iopub.status.busy": "2023-03-26T05:20:03.955318Z",
     "iopub.status.idle": "2023-03-26T05:20:08.155663Z",
     "shell.execute_reply": "2023-03-26T05:20:08.154216Z",
     "shell.execute_reply.started": "2023-03-26T05:20:03.955625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# если у вас есть CUDA, то она понадобится там для экспериментов в catboost\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В данном соревновании участникам предстоит работать с таким форматом данных, как `Parquet`. Узнать подробнее об этом формате Вы можете самостоятельно. Самое главное &ndash; это крайне эффективный бинарный формат сжатия данных по колонокам. Однако, для непосредственной работы с данными и построения моделей нам нужно прочитать их и трансформировать в pandas.DataFrame. При этом сделать это эффективно по памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:08.158201Z",
     "iopub.status.busy": "2023-03-26T05:20:08.157795Z",
     "iopub.status.idle": "2023-03-26T05:20:08.201903Z",
     "shell.execute_reply": "2023-03-26T05:20:08.199942Z",
     "shell.execute_reply.started": "2023-03-26T05:20:08.158160Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_data\"\n",
    "TEST_DATA_PATH = \"/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/test_data\"\n",
    "\n",
    "TRAIN_TARGET_PATH = \"/kaggle/input/alfa-bank-pd-credit-history/data_for_competition/train_target.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:10.762783Z",
     "iopub.status.busy": "2023-03-26T05:20:10.762156Z",
     "iopub.status.idle": "2023-03-26T05:20:19.334970Z",
     "shell.execute_reply": "2023-03-26T05:20:19.333651Z",
     "shell.execute_reply.started": "2023-03-26T05:20:10.762744Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import Model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "import tqdm\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "\n",
    "def pad_sequence(array: np.ndarray, max_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Принимает на вход массив массивов ``array`` и производит padding каждого вложенного массива до ``max_len``.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    array: numpy.ndarray\n",
    "        Входной массив массивов.\n",
    "    max_len: int\n",
    "        Длина, до которой нужно сделать padding вложенных массивов.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    output: numpy.ndarray\n",
    "        Выходной массив.\n",
    "    \"\"\"\n",
    "    if isinstance(max_len, float):\n",
    "        print(max_len)\n",
    "    output = np.zeros((len(features), max_len))\n",
    "    output[:, :array.shape[1]] = array\n",
    "    return output\n",
    "\n",
    "\n",
    "def truncate(x, num_last_credits: int = 0):\n",
    "    return pd.Series({\"sequences\": x.values.transpose()[:, -num_last_credits:]})\n",
    "\n",
    "\n",
    "def transform_credits_to_sequences(credits_frame: pd.DataFrame,\n",
    "                                   num_last_credits: int = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Принимает pandas.DataFrame с записями кредитных историй клиентов, сортирует кредиты по клиентам\n",
    "    (внутри клиента сортирует кредиты от старых к новым), берет ``num_last_credits`` кредитов,\n",
    "    возвращает новый pandas.DataFrame с двумя колонками: id и sequences.\n",
    "    Каждое значение в столбце sequences - это массив массивов.\n",
    "    Каждый вложенный массив - значение одного признака во всех кредитах клиента.\n",
    "    Всего признаков len(features), поэтому будет len(features) массивов.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    credits_frame: pandas.DataFrame\n",
    "        Датафрейм с записями кредитных историй клиентов.\n",
    "    num_last_credits: int, default=0\n",
    "         Количество кредитов клиента, которые будут включены в выходные данные. Если 0, то берутся все кредиты.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    output: pandas.DataFrame\n",
    "        Выходной датафрейм с двумя столбцами: \"id\", \"sequences\".\n",
    "    \"\"\"\n",
    "    return credits_frame \\\n",
    "        .sort_values([\"id\", \"rn\"]) \\\n",
    "        .groupby([\"id\"])[features] \\\n",
    "        .apply(lambda x: truncate(x, num_last_credits=num_last_credits)) \\\n",
    "        .reset_index()\n",
    "\n",
    "\n",
    "def create_padded_buckets(frame_of_sequences: pd.DataFrame, bucket_info: Dict[int, int],\n",
    "                          save_to_file_path: str = None, has_target: bool = True):\n",
    "    \"\"\"\n",
    "    Реализует Sequence Bucketing технику для обучения рекуррентных нейронных сетей.\n",
    "    Принимает на вход датафрейм ``frame_of_sequences`` с двумя столбцами: \"id\", \"sequences\"\n",
    "    (результат работы функции transform_credits_to_sequences),\n",
    "    словарь ``bucket_info``, где для последовательности каждой длины указано, до какой максимальной длины нужно делать\n",
    "    padding, группирует кредиты по бакетам (на основе длины), производит padding нулями и сохраняет результат\n",
    "    в pickle файл, если требуется.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    frame_of_sequences: pandas.DataFrame\n",
    "        Входной датафрейм с двумя столбцами \"id\", \"sequences\" (результат работы функции transform_credits_to_sequences).\n",
    "    bucket_info: Dict[int, int]\n",
    "        Cловарь, где для последовательности каждой длины указано, до какой максимальной длины нужно делать padding.\n",
    "    save_to_file_path: str, default=None\n",
    "        Опциональный путь до файла, куда нужно сохранить результат. Если None, то сохранение не требуется.\n",
    "    has_target: bool, deafult=True\n",
    "        Флаг, есть ли в frame_of_sequences целевая переменная или нет. Если есть, то она также будет записана в выходной словарь.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    dict_result: dict\n",
    "        Выходной словарь со ключами:  \"id\", \"padded_sequences\", \"target\".\n",
    "    \"\"\"\n",
    "    frame_of_sequences[\"sequence_length\"] = frame_of_sequences[\"sequences\"].apply(lambda x: len(x[1]))\n",
    "    frame_of_sequences[\"bucket_idx\"] = frame_of_sequences[\"sequence_length\"].map(bucket_info)\n",
    "    padded_seq = []\n",
    "    targets = []\n",
    "    ids = []\n",
    "\n",
    "    for size, bucket in tqdm(frame_of_sequences.groupby(\"bucket_idx\"), desc=\"Extracting buckets\"):\n",
    "        padded_sequences = bucket[\"sequences\"].apply(lambda x: pad_sequence(x, size)).values\n",
    "        padded_seq.append(np.stack(padded_sequences, axis=0))\n",
    "\n",
    "        if has_target:\n",
    "            targets.append(bucket[\"flag\"].values)\n",
    "\n",
    "        ids.append(bucket[\"id\"].values)\n",
    "\n",
    "    frame_of_sequences.drop(columns=[\"bucket_idx\"], inplace=True)\n",
    "\n",
    "    dict_result = {\n",
    "        \"id\": np.array(ids, dtype=np.object),\n",
    "        \"padded_sequences\": np.array(padded_seq, dtype=np.object),\n",
    "        \"target\": np.array(targets, dtype=np.object) if targets else []\n",
    "    }\n",
    "\n",
    "    if save_to_file_path:\n",
    "        with open(save_to_file_path, \"wb\") as f:\n",
    "            pickle.dump(dict_result, f)\n",
    "    return dict_result\n",
    "\n",
    "\n",
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0, num_parts_to_read: int = 2,\n",
    "                                    columns: List[str] = None, verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Читает ``num_parts_to_read`` партиций и преобразует их к pandas.DataFrame.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    path_to_dataset: str\n",
    "        Путь до директории с партициями.\n",
    "    start_from: int, default=0\n",
    "        Номер партиции, с которой начать чтение.\n",
    "    num_parts_to_read: int, default=2\n",
    "        Число партиций, которые требуется прочитать.\n",
    "    columns: List[str], default=None\n",
    "        Список колонок, которые нужно прочитать из каждой партиции. Если None, то считываются все колонки.\n",
    "\n",
    "    Возвращаемое значение:\n",
    "    ----------------------\n",
    "    frame: pandas.DataFrame\n",
    "        Прочитанные партиции, преобразованные к pandas.DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    start_from = max(0, start_from)\n",
    "    # dictionory of format {partition number: partition filename}\n",
    "    dataset_paths = {int(os.path.splitext(filename)[0].split(\"_\")[-1]): os.path.join(path_to_dataset, filename)\n",
    "                     for filename in os.listdir(path_to_dataset)}\n",
    "    chunks = [dataset_paths[num] for num in sorted(dataset_paths.keys()) if num >= start_from][:num_parts_to_read]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Reading chunks:\", *chunks, sep=\"\\n\")\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        chunk = pd.read_parquet(chunk_path, columns=columns)\n",
    "        res.append(chunk)\n",
    "    return pd.concat(res).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_buckets_from_credits(path_to_dataset, bucket_info, save_to_path, frame_with_ids=None,\n",
    "                                num_parts_to_preprocess_at_once: int = 1,\n",
    "                                num_parts_total=50, has_target=False):\n",
    "    block = 0\n",
    "    for step in tqdm.notebook.tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                                   desc=\"Preparing credit data\"):\n",
    "        credits_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once,\n",
    "                                                        verbose=True)\n",
    "        credits_frame.loc[:, features] += 1\n",
    "        seq = transform_credits_to_sequences(credits_frame)\n",
    "        print(\"Transforming credits to sequences is done.\")\n",
    "\n",
    "        if frame_with_ids is not None:\n",
    "            seq = seq.merge(frame_with_ids, on=\"id\")\n",
    "\n",
    "        block_as_str = str(block)\n",
    "        if len(block_as_str) == 1:\n",
    "            block_as_str = \"00\" + block_as_str\n",
    "        else:\n",
    "            block_as_str = \"0\" + block_as_str\n",
    "\n",
    "        processed_fragment = create_padded_buckets(seq, bucket_info=bucket_info, has_target=has_target,\n",
    "                                                   save_to_file_path=os.path.join(save_to_path,\n",
    "                                                                                  f\"processed_chunk_{block_as_str}.pkl\"))\n",
    "        block += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для примера прочитаем одну партицию в память и оценим, сколько RAM занимает весь DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:19.601634Z",
     "iopub.status.busy": "2023-03-26T05:20:19.601180Z",
     "iopub.status.idle": "2023-03-26T05:20:22.911145Z",
     "shell.execute_reply": "2023-03-26T05:20:22.909783Z",
     "shell.execute_reply.started": "2023-03-26T05:20:19.601579Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:163: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329a6f2b32b24b76be5837895cefe0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем памяти в RAM одной партиции данных с кредитными историями: 0.964 GB\n",
      "Ожидаемый размер в RAM всего датасета: 11.564 GB\n"
     ]
    }
   ],
   "source": [
    "data_frame = read_parquet_dataset_from_local(TRAIN_DATA_PATH, start_from=0, num_parts_to_read=1)\n",
    "\n",
    "memory_usage_of_frame = data_frame.memory_usage(index=True).sum() / 10**9\n",
    "expected_memory_usage = memory_usage_of_frame * 12\n",
    "print(f\"Объем памяти в RAM одной партиции данных с кредитными историями: {round(memory_usage_of_frame, 3)} GB\")\n",
    "print(f\"Ожидаемый размер в RAM всего датасета: {round(expected_memory_usage, 3)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Итого, при чтении всех данных сразу, они займут значительный объем памяти. Решение &ndash; читать данные итеративно небольшими чанками. Чанки организованы таким образом, что для конкретного клиента вся информация о его кредитной истории до момента подачи заявки на кредит расположена внутри одного чанка. Это позволяет загружать данные в память небольшими порциями, выделять все необходимые признаки и получать результирующий фрейм для моделирования. Для этих целей мы предоставляем вам функцию `utils.read_parquet_dataset_from_local`.\n",
    "\n",
    "Так же все чанки данных упорядочены по возрастанию даты заявки на кредит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:22.913931Z",
     "iopub.status.busy": "2023-03-26T05:20:22.912666Z",
     "iopub.status.idle": "2023-03-26T05:20:23.268384Z",
     "shell.execute_reply": "2023-03-26T05:20:23.266410Z",
     "shell.execute_reply.started": "2023-03-26T05:20:22.913877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_frame\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:23.273954Z",
     "iopub.status.busy": "2023-03-26T05:20:23.273427Z",
     "iopub.status.idle": "2023-03-26T05:20:24.123421Z",
     "shell.execute_reply": "2023-03-26T05:20:24.122159Z",
     "shell.execute_reply.started": "2023-03-26T05:20:23.273915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>2999995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>2999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>2999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>2999998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>2999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  flag\n",
       "0              0     0\n",
       "1              1     0\n",
       "2              2     0\n",
       "3              3     0\n",
       "4              4     0\n",
       "...          ...   ...\n",
       "2999995  2999995     0\n",
       "2999996  2999996     0\n",
       "2999997  2999997     0\n",
       "2999998  2999998     0\n",
       "2999999  2999999     0\n",
       "\n",
       "[3000000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = pd.read_csv(TRAIN_TARGET_PATH)\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:24.125856Z",
     "iopub.status.busy": "2023-03-26T05:20:24.125143Z",
     "iopub.status.idle": "2023-03-26T05:20:24.205184Z",
     "shell.execute_reply": "2023-03-26T05:20:24.203654Z",
     "shell.execute_reply.started": "2023-03-26T05:20:24.125805Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=7, random_state=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:20:24.207518Z",
     "iopub.status.busy": "2023-03-26T05:20:24.206936Z",
     "iopub.status.idle": "2023-03-26T05:20:45.959971Z",
     "shell.execute_reply": "2023-03-26T05:20:45.958582Z",
     "shell.execute_reply.started": "2023-03-26T05:20:24.207476Z"
    }
   },
   "outputs": [],
   "source": [
    "for fold_, (train_idx, val_idx) in enumerate(cv.split(train_target)):\n",
    "    train, val = train_target.iloc[train_idx], train_target.iloc[val_idx]\n",
    "    train.to_csv(f\"train_target_fold_{fold_ + 1}.csv\", index=False)\n",
    "    val.to_csv(f\"val_target_fold_{fold_ + 1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
